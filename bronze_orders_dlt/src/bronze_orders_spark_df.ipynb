{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fd75c85-bc26-44bd-84fd-f3888740fdaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Databricks Asset Bundles must be deployed using the Databricks CLI, not from the Databricks UI.\n",
    "\n",
    "Databricks UI doesnot suppport deployment\n",
    "\n",
    "The UI Deploy button only syncs notebooks/files, it does not deploy infrastructure resources like DLT pipelines or jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66927724-fb50-4cbc-8189-e7d590e71bbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import current_timestamp, col\n",
    "\n",
    "SOURCE_TABLE = \"source.default.orders\"\n",
    "\n",
    "BRONZE_CATALOG = \"bronze\"\n",
    "BRONZE_SCHEMA  = \"default\" #Any have anything based on business use case it is an testing schema.\n",
    "BRONZE_TABLE   = \"orders\"\n",
    "\n",
    "@dlt.table( #Delta Live Tables decorator: Creates and manages a table in Catalog\n",
    "    name=f\"{BRONZE_CATALOG}.{BRONZE_SCHEMA}.{BRONZE_TABLE}\",\n",
    "    comment=\"Bronze ingestion from source.default.orders (raw copy) with bronze_ingestion_date.\"\n",
    ")\n",
    "def bronze_orders():\n",
    "    return (\n",
    "        spark.table(SOURCE_TABLE) #Reads Source table\n",
    "        .select(\n",
    "            col(\"id\").cast(\"int\").alias(\"id\"),\n",
    "            col(\"category\").cast(\"string\").alias(\"category\"),\n",
    "            col(\"amount\").cast(\"int\").alias(\"amount\"),\n",
    "            col(\"ingestion_date\").cast(\"timestamp\").alias(\"ingestion_date\")  # from source\n",
    "        )\n",
    "        .withColumn(\"bronze_ingestion_date\", current_timestamp())            # added by bronze\n",
    "    )\n",
    "\n",
    "#df.write.format(\"delta\").mode(\"append\").saveAsTable(\"bronze.default.orders\") {Works in Job} - NOT REQUIRED IN DLT, as dlt decorater handles it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb542a58-e71e-4243-9e89-7428f0a9167f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Other Option without dlt"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, col\n",
    "\n",
    "df = (\n",
    "    spark.table(\"source.default.orders\")\n",
    "    .select(\n",
    "        col(\"id\").cast(\"int\").alias(\"id\"),\n",
    "        col(\"category\").cast(\"string\").alias(\"category\"),\n",
    "        col(\"amount\").cast(\"int\").alias(\"amount\"),\n",
    "        col(\"ingestion_date\").cast(\"timestamp\").alias(\"ingestion_date\")\n",
    "    )\n",
    "    .withColumn(\"bronze_ingestion_date\", current_timestamp())\n",
    ")\n",
    "\n",
    "df.write.format(\"delta\").mode(\"append\").saveAsTable(\"workspace.default.bronze_orders\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_orders_spark_df",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
